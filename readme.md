# üì¶ Pipeline ETL Sin Servidor con AWS Glue y Terraform

## Descripci√≥n General del Proyecto

Este proyecto demuestra la implementaci√≥n y automatizaci√≥n de un pipeline de Extracci√≥n, Transformaci√≥n y Carga (**ETL**) utilizando **AWS Glue** como motor de procesamiento sin servidor (PySpark) y **Terraform** para aprovisionar toda la infraestructura de soporte.

El principal objetivo es procesar archivos CSV crudos (`Raw Data`), aplicarles una transformaci√≥n sencilla (a√±adir una columna de marca de tiempo) y almacenarlos en un formato optimizado (Parquet) en un bucket de datos procesados.

---

## üèõÔ∏è Arquitectura Desplegada

El despliegue de Terraform crea la siguiente infraestructura en su cuenta de AWS:

| **Componente** | **Descripci√≥n** | **Uso** | 
| ----- | ----- | ----- | 
| **S3 Raw Bucket** | Almacenamiento de datos de entrada (CSV). | Origen de datos (`--S3_INPUT_PATH`). | 
| **S3 Processed Bucket** | Almacenamiento de datos transformados (Parquet). | Destino de datos (`--S3_OUTPUT_PATH`). | 
| **S3 Scripts Bucket** | Almacenamiento del c√≥digo PySpark (`etl_job.py`). | Referencia para el Job de Glue. | 
| **IAM Role** | Rol de servicio asumido por Glue. | Otorga permisos para leer S3, escribir S3 y escribir logs en CloudWatch. | 
| **AWS Glue Job** | Trabajo ETL configurado (PySpark 4.0). | Motor de ejecuci√≥n del script de transformaci√≥n. | 

---

## üöÄ Despliegue y Configuraci√≥n

### Prerrequisitos

Aseg√∫rese de tener instalados y configurados los siguientes elementos:

1. **Terraform:** Para el aprovisionamiento de la infraestructura.

2. **AWS CLI:** Para la autenticaci√≥n y la gesti√≥n de S3/Glue.

3. **Credenciales de AWS:** Configuradas localmente para que Terraform y la CLI puedan acceder a la cuenta (v√≠a `aws configure` o variables de entorno).

### Estructura de Archivos

Aseg√∫rese de que su directorio local tenga la siguiente estructura antes de ejecutar Terraform:

/glue_pipeline_project
‚îú‚îÄ‚îÄ main.tf             # Definici√≥n principal de Glue Job, IAM y S3
‚îú‚îÄ‚îÄ variables.tf        # Definici√≥n de variables de entrada (proyecto_nombre, regi√≥n)
‚îú‚îÄ‚îÄ outputs.tf          # Salidas de Terraform (ARN de buckets, etc.)
‚îú‚îÄ‚îÄ sample_data.csv     # Archivo de prueba para la validaci√≥n
‚îî‚îÄ‚îÄ glue_scripts/
‚îî‚îÄ‚îÄ etl_job.py      # C√≥digo PySpark con la l√≥gica de transformaci√≥n


### Pasos de Despliegue

1. **Inicializar Terraform:**

   ```bash
   terraform init
Planificar (Opcional, pero recomendado):

Bash

terraform plan -var="proyecto_nombre=datos_ventas"
Aplicar el Despliegue:

Bash

terraform apply -var="proyecto_nombre=datos_ventas"
Aseg√∫rese de reemplazar datos_ventas por el nombre de proyecto que desee.

‚úÖ Ejecuci√≥n y Validaci√≥n del Pipeline
Una vez que Terraform haya finalizado con √©xito, el pipeline est√° listo, pero el Job de Glue debe ser activado.

1. Inyecci√≥n de Datos
Utilice la AWS CLI para subir el archivo de prueba al bucket de entrada (RAW).

Bash

# Reemplace <NOMBRE_BUCKET_RAW> con el output de Terraform
aws s3 cp sample_data.csv s3://<NOMBRE_BUCKET_RAW>/input/sample_data.csv
2. Ejecutar el Job de Glue
Vaya a la consola de AWS Glue, busque el trabajo con el nombre datos_ventas-etl-job (o el nombre que haya definido en la variable proyecto_nombre) y haga clic en "Run job".

3. Verificar el Resultado
Monitoree el estado del Job en la consola.

Una vez que el Job finalice (SUCCEEDED), navegue al Processed Bucket (Output) en S3.

Deber√≠a encontrar un nuevo archivo Parquet. Este archivo contendr√° los datos originales m√°s la columna load_date a√±adida por el script PySpark, confirmando la transformaci√≥n.

üóëÔ∏è Limpieza de Recursos
Para evitar costes, aseg√∫rese de eliminar todos los recursos creados por Terraform:

Bash

terraform destroy -var="proyecto_nombre=datos_ventas"